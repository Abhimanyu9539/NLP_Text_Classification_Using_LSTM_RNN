{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import the required packages\n",
        "!pip install nltk==3.6.1\n",
        "!pip install numpy==1.18.5\n",
        "!pip install pandas==1.3.0\n",
        "!pip install torch==1.9.0\n",
        "!pip install tqdm==4.59.0\n",
        "!pip install scikit_learn==1.0.2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaTI-Dn1RTK6",
        "outputId": "a461354a-74ed-414e-e344-14d8da7091cc"
      },
      "id": "eaTI-Dn1RTK6",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk==3.6.1 in /usr/local/lib/python3.7/dist-packages (3.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.1) (1.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.1) (4.59.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.1) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from nltk==3.6.1) (2019.12.20)\n",
            "Requirement already satisfied: numpy==1.18.5 in /usr/local/lib/python3.7/dist-packages (1.18.5)\n",
            "Requirement already satisfied: pandas==1.3.0 in /usr/local/lib/python3.7/dist-packages (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (1.18.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.3.0) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.3.0) (1.15.0)\n",
            "Requirement already satisfied: torch==1.9.0 in /usr/local/lib/python3.7/dist-packages (1.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm==4.59.0 in /usr/local/lib/python3.7/dist-packages (4.59.0)\n",
            "Requirement already satisfied: scikit_learn==1.0.2 in /usr/local/lib/python3.7/dist-packages (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==1.0.2) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==1.0.2) (1.18.5)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==1.0.2) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit_learn==1.0.2) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ccf15cd",
      "metadata": {
        "id": "6ccf15cd"
      },
      "outputs": [],
      "source": [
        "# import the required libraries\n",
        "import re\n",
        "import torch\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5547d608",
      "metadata": {
        "id": "5547d608"
      },
      "outputs": [],
      "source": [
        "# define configuration file paths\n",
        "lr = 0.0001\n",
        "input_size = 50\n",
        "num_epochs = 50\n",
        "hidden_size = 50\n",
        "label_col = \"Product\"\n",
        "tokens_path = \"Output/tokens.pkl\"\n",
        "labels_path = \"Output/labels.pkl\"\n",
        "data_path = \"Input/complaints.csv\"\n",
        "rnn_model_path = \"Output/model_rnn.pth\"\n",
        "lstm_model_path = \"Output/model_lstm.pth\"\n",
        "vocabulary_path = \"Output/vocabulary.pkl\"\n",
        "embeddings_path = \"Output/embeddings.pkl\"\n",
        "glove_vector_path = \"Input/glove.6B.50d.txt\"\n",
        "text_col_name = \"Consumer complaint narrative\"\n",
        "label_encoder_path = \"Output/label_encoder.pkl\"\n",
        "product_map = {'Vehicle loan or lease': 'vehicle_loan',\n",
        "               'Credit reporting, credit repair services, or other personal consumer reports': 'credit_report',\n",
        "               'Credit card or prepaid card': 'card',\n",
        "               'Money transfer, virtual currency, or money service': 'money_transfer',\n",
        "               'virtual currency': 'money_transfer',\n",
        "               'Mortgage': 'mortgage',\n",
        "               'Payday loan, title loan, or personal loan': 'loan',\n",
        "               'Debt collection': 'debt_collection',\n",
        "               'Checking or savings account': 'savings_account',\n",
        "               'Credit card': 'card',\n",
        "               'Bank account or service': 'savings_account',\n",
        "               'Credit reporting': 'credit_report',\n",
        "               'Prepaid card': 'card',\n",
        "               'Payday loan': 'loan',\n",
        "               'Other financial service': 'others',\n",
        "               'Virtual currency': 'money_transfer',\n",
        "               'Student loan': 'loan',\n",
        "               'Consumer Loan': 'loan',\n",
        "               'Money transfers': 'money_transfer'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9935336a",
      "metadata": {
        "id": "9935336a"
      },
      "outputs": [],
      "source": [
        "# define function for saving a file\n",
        "def save_file(name, obj):\n",
        "    \"\"\"\n",
        "    Function to save an object as pickle file\n",
        "    \"\"\"\n",
        "    with open(name, 'wb') as f:\n",
        "        pickle.dump(obj, f)\n",
        "\n",
        "# define function for loading a file\n",
        "def load_file(name):\n",
        "    \"\"\"\n",
        "    Function to load a pickle object\n",
        "    \"\"\"\n",
        "    return pickle.load(open(name, \"rb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cde95503",
      "metadata": {
        "id": "cde95503"
      },
      "source": [
        "## Process glove embeddings\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "531f3f24",
      "metadata": {
        "id": "531f3f24"
      },
      "outputs": [],
      "source": [
        "# open the glove embeddings file and read\n",
        "with open(glove_vector_path, \"rt\") as f:\n",
        "    emb = f.readlines()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03d4e03a",
      "metadata": {
        "id": "03d4e03a"
      },
      "source": [
        "### 400000 unique words are there in the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60b0e62",
      "metadata": {
        "id": "b60b0e62"
      },
      "outputs": [],
      "source": [
        "# length of embeddings\n",
        "len(emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fb6e904",
      "metadata": {
        "id": "7fb6e904"
      },
      "source": [
        "### Check the first record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e27f3bfd",
      "metadata": {
        "id": "e27f3bfd"
      },
      "outputs": [],
      "source": [
        "# check first record\n",
        "emb[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d5698d0",
      "metadata": {
        "id": "3d5698d0"
      },
      "outputs": [],
      "source": [
        "# split the first record and check for vocabulary\n",
        "emb[0].split()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89c86e32",
      "metadata": {
        "id": "89c86e32"
      },
      "outputs": [],
      "source": [
        "# split the first record and check for embeddings\n",
        "emb[0].split()[1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84fdfd89",
      "metadata": {
        "id": "84fdfd89"
      },
      "source": [
        "### Separate embeddings and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "242f889d",
      "metadata": {
        "id": "242f889d"
      },
      "outputs": [],
      "source": [
        "vocabulary, embeddings = [], []\n",
        "\n",
        "for item in emb:\n",
        "    vocabulary.append(item.split()[0])\n",
        "    embeddings.append(item.split()[1:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbcb6c39",
      "metadata": {
        "id": "bbcb6c39"
      },
      "source": [
        "### Convert embeddings to numpy float array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bfab44e",
      "metadata": {
        "id": "8bfab44e"
      },
      "outputs": [],
      "source": [
        "embeddings = np.array(embeddings, dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01efb524",
      "metadata": {
        "id": "01efb524"
      },
      "outputs": [],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99dbd776",
      "metadata": {
        "id": "99dbd776"
      },
      "source": [
        "### Add embeddings for padding and unknown items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85da0e52",
      "metadata": {
        "id": "85da0e52"
      },
      "outputs": [],
      "source": [
        "vocabulary[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fbb2c52",
      "metadata": {
        "id": "0fbb2c52"
      },
      "outputs": [],
      "source": [
        "vocabulary = [\"<pad>\", \"<unk>\"] + vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "428314b9",
      "metadata": {
        "id": "428314b9"
      },
      "outputs": [],
      "source": [
        "embeddings = np.vstack([np.ones(50, dtype=np.float32), np.mean(embeddings, axis=0),\n",
        "                            embeddings])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cac06f0",
      "metadata": {
        "id": "5cac06f0"
      },
      "outputs": [],
      "source": [
        "print(len(vocabulary), embeddings.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42b3b07a",
      "metadata": {
        "id": "42b3b07a"
      },
      "source": [
        "### Save embeddings and vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b173a0ea",
      "metadata": {
        "id": "b173a0ea"
      },
      "outputs": [],
      "source": [
        "save_file(embeddings_path, embeddings)\n",
        "save_file(vocabulary_path, vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f58d0479",
      "metadata": {
        "id": "f58d0479"
      },
      "source": [
        "## Process text data\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f567f06f",
      "metadata": {
        "id": "f567f06f"
      },
      "source": [
        "### Read the data file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c71c7dfc",
      "metadata": {
        "id": "c71c7dfc"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(data_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86dffa9a",
      "metadata": {
        "id": "86dffa9a"
      },
      "source": [
        "### Drop rows where the text column is empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "216c4a4d",
      "metadata": {
        "id": "216c4a4d"
      },
      "outputs": [],
      "source": [
        "data.dropna(subset=[text_col_name], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a70b83e2",
      "metadata": {
        "id": "a70b83e2"
      },
      "source": [
        "### Replace duplicate labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64801543",
      "metadata": {
        "id": "64801543"
      },
      "outputs": [],
      "source": [
        "data.replace({label_col: product_map}, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b1714dc",
      "metadata": {
        "id": "6b1714dc"
      },
      "source": [
        "### Encode the label column and save the encoder and encoded labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "900db623",
      "metadata": {
        "id": "900db623"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "label_encoder.fit(data[label_col])\n",
        "labels = label_encoder.transform(data[label_col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "edd80694",
      "metadata": {
        "id": "edd80694"
      },
      "outputs": [],
      "source": [
        "labels[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52c367c8",
      "metadata": {
        "id": "52c367c8"
      },
      "outputs": [],
      "source": [
        "label_encoder.classes_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7ccce6f",
      "metadata": {
        "id": "f7ccce6f"
      },
      "outputs": [],
      "source": [
        "data[label_col]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0544906f",
      "metadata": {
        "id": "0544906f"
      },
      "outputs": [],
      "source": [
        "save_file(labels_path, labels)\n",
        "save_file(label_encoder_path, label_encoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc16fc63",
      "metadata": {
        "id": "fc16fc63"
      },
      "source": [
        "### Process the text column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06b2e29d",
      "metadata": {
        "id": "06b2e29d"
      },
      "outputs": [],
      "source": [
        "input_text = data[text_col_name]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2fd241b",
      "metadata": {
        "id": "e2fd241b"
      },
      "source": [
        "### Convert text to lower case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857dec2b",
      "metadata": {
        "id": "857dec2b"
      },
      "outputs": [],
      "source": [
        "input_text = [i.lower() for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54188fc7",
      "metadata": {
        "id": "54188fc7"
      },
      "source": [
        "### Remove punctuations except apostrophe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee07ba3",
      "metadata": {
        "id": "eee07ba3"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(r\"[^\\w\\d'\\s]+\", \" \", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03cce62f",
      "metadata": {
        "id": "03cce62f"
      },
      "source": [
        "### Remove digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c239f6ec",
      "metadata": {
        "id": "c239f6ec"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(\"\\d+\", \"\", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa2608b",
      "metadata": {
        "id": "7fa2608b"
      },
      "source": [
        "### Remove more than one consecutive instance of 'x'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "264dc356",
      "metadata": {
        "id": "264dc356"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(r'[x]{2,}', \"\", i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dea776b",
      "metadata": {
        "id": "6dea776b"
      },
      "source": [
        "### Replace multiple spaces with single space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd744c4",
      "metadata": {
        "id": "1cd744c4"
      },
      "outputs": [],
      "source": [
        "input_text = [re.sub(' +', ' ', i) for i in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934505ad",
      "metadata": {
        "id": "934505ad"
      },
      "source": [
        "### Tokenize the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d0c315fb",
      "metadata": {
        "id": "d0c315fb"
      },
      "outputs": [],
      "source": [
        "tokens = [word_tokenize(t) for t in tqdm(input_text)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c312593",
      "metadata": {
        "id": "5c312593"
      },
      "source": [
        "### Take the first 20 tokens in each complaint text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7627e35b",
      "metadata": {
        "id": "7627e35b"
      },
      "outputs": [],
      "source": [
        "tokens = [i[:20] if len(i) > 19 else ['<pad>'] * (20 - len(i)) + i for i in tqdm(tokens)]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be6561df",
      "metadata": {
        "id": "be6561df"
      },
      "source": [
        "### Convert tokens to integer indices from vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "288a4e01",
      "metadata": {
        "id": "288a4e01"
      },
      "outputs": [],
      "source": [
        "def token_index(tokens, vocabulary, missing='<unk>'):\n",
        "    \"\"\"\n",
        "    :param tokens: List of word tokens\n",
        "    :param vocabulary: All words in the embeddings\n",
        "    :param missing: Token for words not present in the vocabulary\n",
        "    :return: List of integers representing the word tokens\n",
        "    \"\"\"\n",
        "    idx_token = []\n",
        "    for text in tqdm(tokens):\n",
        "        idx_text = []\n",
        "        for token in text:\n",
        "            if token in vocabulary:\n",
        "                idx_text.append(vocabulary.index(token))\n",
        "            else:\n",
        "                idx_text.append(vocabulary.index(missing))\n",
        "        idx_token.append(idx_text)\n",
        "    return idx_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "618f32df",
      "metadata": {
        "id": "618f32df"
      },
      "outputs": [],
      "source": [
        "tokens = token_index(tokens, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c51657a5",
      "metadata": {
        "id": "c51657a5"
      },
      "outputs": [],
      "source": [
        "len(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7fe16b6",
      "metadata": {
        "id": "e7fe16b6"
      },
      "outputs": [],
      "source": [
        "tokens[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00b35a7e",
      "metadata": {
        "id": "00b35a7e"
      },
      "outputs": [],
      "source": [
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a056489b",
      "metadata": {
        "id": "a056489b"
      },
      "outputs": [],
      "source": [
        "vocabulary[tokens[0][0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab9c8712",
      "metadata": {
        "id": "ab9c8712"
      },
      "source": [
        "### Save the tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab83d5fb",
      "metadata": {
        "id": "ab83d5fb"
      },
      "outputs": [],
      "source": [
        "save_file(tokens_path, tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d52e1c1",
      "metadata": {
        "id": "8d52e1c1"
      },
      "source": [
        "## Create PyTorch Dataset\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8936a733",
      "metadata": {
        "id": "8936a733"
      },
      "outputs": [],
      "source": [
        "class TextDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokens, embeddings, labels):\n",
        "        \"\"\"\n",
        "        :param tokens: List of word tokens\n",
        "        :param embeddings: Word embeddings (from glove)\n",
        "        :param labels: List of labels\n",
        "        \"\"\"\n",
        "        self.tokens = tokens\n",
        "        self.embeddings = embeddings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokens)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.labels[idx], self.embeddings[self.tokens[idx], :]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f1c8577",
      "metadata": {
        "id": "8f1c8577"
      },
      "source": [
        "## Create Models\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b77dedc",
      "metadata": {
        "id": "8b77dedc"
      },
      "source": [
        "### RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4953da5",
      "metadata": {
        "id": "f4953da5"
      },
      "outputs": [],
      "source": [
        "class RNNNetwork(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        :param input_size: Size of embedding\n",
        "        :param hidden_size: Hidden vector size\n",
        "        :param num_classes: Number of classes in the dataset\n",
        "        \"\"\"\n",
        "        super(RNNNetwork, self).__init__()\n",
        "        # RNN Layer\n",
        "        self.rnn = torch.nn.RNN(input_size=input_size,\n",
        "                                hidden_size=hidden_size,\n",
        "                                batch_first=True)\n",
        "        # Linear Layer\n",
        "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        _, hidden = self.rnn(input_data)\n",
        "        output = self.linear(hidden)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "550fc77e",
      "metadata": {
        "id": "550fc77e"
      },
      "source": [
        "### LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1383a39",
      "metadata": {
        "id": "a1383a39"
      },
      "outputs": [],
      "source": [
        "class LSTMNetwork(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        \"\"\"\n",
        "        :param input_size: Size of embedding\n",
        "        :param hidden_size: Hidden vector size\n",
        "        :param num_classes: Number of classes in the dataset\n",
        "        \"\"\"\n",
        "        super(LSTMNetwork, self).__init__()\n",
        "        # LSTM Layer\n",
        "        self.rnn = torch.nn.LSTM(input_size=input_size,\n",
        "                                 hidden_size=hidden_size,\n",
        "                                 batch_first=True)\n",
        "        # Linear Layer\n",
        "        self.linear = torch.nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, input_data):\n",
        "        _, (hidden, _) = self.rnn(input_data)\n",
        "        output = self.linear(hidden[-1])\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76960f7",
      "metadata": {
        "id": "e76960f7"
      },
      "source": [
        "### Define train function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f289dabc",
      "metadata": {
        "id": "f289dabc"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, valid_loader, model, criterion, optimizer, device,\n",
        "          num_epochs, model_path):\n",
        "    \"\"\"\n",
        "    Function to train the model\n",
        "    :param train_loader: Data loader for train dataset\n",
        "    :param valid_loader: Data loader for validation dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param optimizer: Optimizer\n",
        "    :param device: CUDA or CPU\n",
        "    :param num_epochs: Number of epochs\n",
        "    :param model_path: Path to save the model\n",
        "    \"\"\"\n",
        "    best_loss = 1e8\n",
        "    for i in range(num_epochs):\n",
        "        print(f\"Epoch {i+1} of {num_epochs}\")\n",
        "        valid_loss, train_loss = [], []\n",
        "        model.train()\n",
        "        # Train loop\n",
        "        for batch_labels, batch_data in tqdm(train_loader):\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            batch_labels = batch_labels.type(torch.LongTensor)\n",
        "            batch_data = batch_data.to(device)\n",
        "            # Forward pass\n",
        "            batch_output = model(batch_data)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            train_loss.append(loss.item())\n",
        "            optimizer.zero_grad()\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            # Gradient update step\n",
        "            optimizer.step()\n",
        "        model.eval()\n",
        "        # Validation loop\n",
        "        for batch_labels, batch_data in tqdm(valid_loader):\n",
        "            # Move data to GPU if available\n",
        "            batch_labels = batch_labels.to(device)\n",
        "            batch_labels = batch_labels.type(torch.LongTensor)\n",
        "            batch_data = batch_data.to(device)\n",
        "            # Forward pass\n",
        "            batch_output = model(batch_data)\n",
        "            batch_output = torch.squeeze(batch_output)\n",
        "            # Calculate loss\n",
        "            loss = criterion(batch_output, batch_labels)\n",
        "            valid_loss.append(loss.item())\n",
        "        t_loss = np.mean(train_loss)\n",
        "        v_loss = np.mean(valid_loss)\n",
        "        print(f\"Train Loss: {t_loss}, Validation Loss: {v_loss}\")\n",
        "        if v_loss < best_loss:\n",
        "            best_loss = v_loss\n",
        "            # Save model if validation loss improves\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        print(f\"Best Validation Loss: {best_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "853388ef",
      "metadata": {
        "id": "853388ef"
      },
      "source": [
        "### Define test function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b9e405f",
      "metadata": {
        "id": "6b9e405f"
      },
      "outputs": [],
      "source": [
        "def test(test_loader, model, criterion, device):\n",
        "    \"\"\"\n",
        "    Function to test the model\n",
        "    :param test_loader: Data loader for test dataset\n",
        "    :param model: Model object\n",
        "    :param criterion: Loss function\n",
        "    :param device: CUDA or CPU\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    test_loss = []\n",
        "    test_accu = []\n",
        "    for batch_labels, batch_data in tqdm(test_loader):\n",
        "        # Move data to device\n",
        "        batch_labels = batch_labels.to(device)\n",
        "        batch_labels = batch_labels.type(torch.LongTensor)\n",
        "        batch_data = batch_data.to(device)\n",
        "        # Forward pass\n",
        "        batch_output = model(batch_data)\n",
        "        batch_output = torch.squeeze(batch_output)\n",
        "        # Calculate loss\n",
        "        loss = criterion(batch_output, batch_labels)\n",
        "        test_loss.append(loss.item())\n",
        "        batch_preds = torch.argmax(batch_output, axis=1)\n",
        "        # Move predictions to CPU\n",
        "        if torch.cuda.is_available():\n",
        "            batch_labels = batch_labels.cpu()\n",
        "            batch_preds = batch_preds.cpu()\n",
        "        # Compute accuracy\n",
        "        test_accu.append(accuracy_score(batch_labels.detach().numpy(),\n",
        "                                        batch_preds.detach().numpy()))\n",
        "    test_loss = np.mean(test_loss)\n",
        "    test_accu = np.mean(test_accu)\n",
        "    print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accu}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4163f818",
      "metadata": {
        "id": "4163f818"
      },
      "source": [
        "## Train RNN Model\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e752fd29",
      "metadata": {
        "id": "e752fd29"
      },
      "source": [
        "### Load the files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9aaae86",
      "metadata": {
        "id": "c9aaae86"
      },
      "outputs": [],
      "source": [
        "tokens = load_file(tokens_path)\n",
        "labels = load_file(labels_path)\n",
        "embeddings = load_file(embeddings_path)\n",
        "label_encoder = load_file(label_encoder_path)\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ad46f7e",
      "metadata": {
        "id": "2ad46f7e"
      },
      "source": [
        "### Split data into train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7916b320",
      "metadata": {
        "id": "7916b320"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tokens, labels,\n",
        "                                                    test_size=0.2)\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train,\n",
        "                                                      test_size=0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "171cc1be",
      "metadata": {
        "id": "171cc1be"
      },
      "source": [
        "### Create PyTorch datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af22498f",
      "metadata": {
        "id": "af22498f"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextDataset(X_train, embeddings, y_train)\n",
        "valid_dataset = TextDataset(X_valid, embeddings, y_valid)\n",
        "test_dataset = TextDataset(X_test, embeddings, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "030610ea",
      "metadata": {
        "id": "030610ea"
      },
      "source": [
        "### Create data loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0dcc6dd",
      "metadata": {
        "id": "a0dcc6dd"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16,\n",
        "                                           shuffle=True, drop_last=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af02c4c2",
      "metadata": {
        "id": "af02c4c2"
      },
      "source": [
        "### Create model object"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7aaacda1",
      "metadata": {
        "id": "7aaacda1"
      },
      "outputs": [],
      "source": [
        "model = RNNNetwork(input_size, hidden_size, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c88a3f8",
      "metadata": {
        "id": "7c88a3f8"
      },
      "source": [
        "### Move the model to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca465cd6",
      "metadata": {
        "id": "ca465cd6"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1deebde4",
      "metadata": {
        "id": "1deebde4"
      },
      "source": [
        "### Define loss function and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4becc10",
      "metadata": {
        "id": "d4becc10"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f90ad48",
      "metadata": {
        "id": "7f90ad48"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f80daf65",
      "metadata": {
        "id": "f80daf65"
      },
      "outputs": [],
      "source": [
        "train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "      device, num_epochs, rnn_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1215b67c",
      "metadata": {
        "id": "1215b67c"
      },
      "source": [
        "## Train LSTM Model\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5e3ceb0",
      "metadata": {
        "id": "a5e3ceb0"
      },
      "outputs": [],
      "source": [
        "model = LSTMNetwork(input_size, hidden_size, num_classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a8e560",
      "metadata": {
        "id": "92a8e560"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7aee94f",
      "metadata": {
        "id": "d7aee94f"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f9d0c17",
      "metadata": {
        "id": "9f9d0c17"
      },
      "outputs": [],
      "source": [
        "train(train_loader, valid_loader, model, criterion, optimizer,\n",
        "      device, num_epochs, lstm_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1882978",
      "metadata": {
        "id": "f1882978"
      },
      "outputs": [],
      "source": [
        "test(test_loader, model, criterion, device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a74eb71",
      "metadata": {
        "id": "8a74eb71"
      },
      "source": [
        "## Predict on new text\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff13ab78",
      "metadata": {
        "id": "ff13ab78"
      },
      "outputs": [],
      "source": [
        "input_text = '''I am a victim of Identity Theft & currently have an Experian account that \n",
        "I can view my Experian Credit Report and getting notified when there is activity on \n",
        "my Experian Credit Report. For the past 3 days I've spent a total of approximately 9 \n",
        "hours on the phone with Experian. Every time I call I get transferred repeatedly and \n",
        "then my last transfer and automated message states to press 1 and leave a message and \n",
        "someone would call me. Every time I press 1 I get an automatic message stating than you \n",
        "before I even leave a message and get disconnected. I call Experian again, explain what \n",
        "is happening and the process begins again with the same end result. I was trying to have \n",
        "this issue attended and resolved informally but I give up after 9 hours. There are hard \n",
        "hit inquiries on my Experian Credit Report that are fraud, I didn't authorize, or recall \n",
        "and I respectfully request that Experian remove the hard hit inquiries immediately just \n",
        "like they've done in the past when I was able to speak to a live Experian representative \n",
        "in the United States. The following are the hard hit inquiries : BK OF XXXX XX/XX/XXXX \n",
        "XXXX XXXX XXXX  XX/XX/XXXX XXXX  XXXX XXXX  XX/XX/XXXX XXXX  XX/XX/XXXX XXXX  XXXX \n",
        "XX/XX/XXXX'''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "678d4e28",
      "metadata": {
        "id": "678d4e28"
      },
      "source": [
        "### Process input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dded0d3",
      "metadata": {
        "id": "1dded0d3"
      },
      "outputs": [],
      "source": [
        "input_text = input_text.lower()\n",
        "input_text = re.sub(r\"[^\\w\\d'\\s]+\", \" \", input_text)\n",
        "input_text = re.sub(\"\\d+\", \"\", input_text)\n",
        "input_text = re.sub(r'[x]{2,}', \"\", input_text)\n",
        "input_text = re.sub(' +', ' ', input_text)\n",
        "tokens = word_tokenize(input_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d09d91b1",
      "metadata": {
        "id": "d09d91b1"
      },
      "source": [
        "### Add padding if the length of tokens is less than 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ba95fe0",
      "metadata": {
        "id": "7ba95fe0"
      },
      "outputs": [],
      "source": [
        "tokens = ['<pad>']*(20-len(tokens))+tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82fbbab5",
      "metadata": {
        "id": "82fbbab5"
      },
      "source": [
        "### Tokenize the input text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821fb5de",
      "metadata": {
        "id": "821fb5de"
      },
      "outputs": [],
      "source": [
        "idx_token = []\n",
        "for token in tokens:\n",
        "    if token in vocabulary:\n",
        "        idx_token.append(vocabulary.index(token))\n",
        "    else:\n",
        "        idx_token.append(vocabulary.index('<unk>'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540cb058",
      "metadata": {
        "id": "540cb058"
      },
      "source": [
        "### Get embeddings for tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d6cc01c",
      "metadata": {
        "id": "1d6cc01c"
      },
      "outputs": [],
      "source": [
        "token_emb = embeddings[idx_token,:]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de584626",
      "metadata": {
        "id": "de584626"
      },
      "source": [
        "### Convert to torch tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "780d42e4",
      "metadata": {
        "id": "780d42e4"
      },
      "outputs": [],
      "source": [
        "inp = torch.from_numpy(token_emb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e91e281c",
      "metadata": {
        "id": "e91e281c"
      },
      "source": [
        "### Move the tensor to GPU if available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f15e636d",
      "metadata": {
        "id": "f15e636d"
      },
      "outputs": [],
      "source": [
        "inp = inp.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebed4640",
      "metadata": {
        "id": "ebed4640"
      },
      "source": [
        "### Create a batch of one record"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c3865b5",
      "metadata": {
        "id": "5c3865b5"
      },
      "outputs": [],
      "source": [
        "inp = torch.unsqueeze(inp, 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2cc919d",
      "metadata": {
        "id": "e2cc919d"
      },
      "source": [
        "### Load label encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92a17e27",
      "metadata": {
        "id": "92a17e27"
      },
      "outputs": [],
      "source": [
        "label_encoder = load_file(label_encoder_path)\n",
        "num_classes = len(label_encoder.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07dd7955",
      "metadata": {
        "id": "07dd7955"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbe841b",
      "metadata": {
        "id": "cdbe841b"
      },
      "source": [
        "### RNN prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a23eb5c",
      "metadata": {
        "id": "1a23eb5c"
      },
      "outputs": [],
      "source": [
        "# Create model object\n",
        "model = RNNNetwork(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(rnn_model_path))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    \n",
        "# Forward pass\n",
        "out = torch.squeeze(model(inp))\n",
        "\n",
        "# Find predicted class\n",
        "prediction = label_encoder.classes_[torch.argmax(out)]\n",
        "print(f\"Predicted  Class: {prediction}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1426d72f",
      "metadata": {
        "id": "1426d72f"
      },
      "source": [
        "### LSTM prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db3cce1e",
      "metadata": {
        "id": "db3cce1e"
      },
      "outputs": [],
      "source": [
        "# Create model object\n",
        "model = LSTMNetwork(input_size, hidden_size, num_classes)\n",
        "\n",
        "# Load trained weights\n",
        "model.load_state_dict(torch.load(lstm_model_path))\n",
        "\n",
        "# Move the model to GPU if available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    \n",
        "# Forward pass\n",
        "out = torch.squeeze(model(inp))\n",
        "\n",
        "# Find predicted class\n",
        "prediction = label_encoder.classes_[torch.argmax(out)]\n",
        "print(f\"Predicted  Class: {prediction}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290e2616",
      "metadata": {
        "id": "290e2616"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "99dbd776",
        "42b3b07a",
        "f567f06f",
        "86dffa9a",
        "a70b83e2",
        "6b1714dc",
        "fc16fc63",
        "e2fd241b",
        "54188fc7",
        "03cce62f",
        "7fa2608b",
        "6dea776b",
        "934505ad",
        "5c312593",
        "be6561df",
        "ab9c8712",
        "8b77dedc",
        "550fc77e",
        "e752fd29",
        "2ad46f7e",
        "171cc1be",
        "030610ea",
        "af02c4c2",
        "7c88a3f8",
        "1deebde4",
        "7f90ad48",
        "678d4e28",
        "d09d91b1",
        "82fbbab5",
        "540cb058",
        "de584626",
        "e91e281c",
        "ebed4640",
        "e2cc919d",
        "cdbe841b",
        "1426d72f"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}